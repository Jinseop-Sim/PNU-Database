# Clustering
---
> 한국말로는 군집화 라고 표현한다.  

- Example
  - 백만장자 거주지 Data ==> 베버리 힐스나 맨해튼에 군집
  - 유권자 데이터 ==> 은퇴자, 젊은 백수 등으로 군집
- 군집화를 어떻게 시키는 지에는 정답이 없다.
  - 대학원생을 젊은 백수와 같이 묶을 수도 있다.
- 왜 군집화를 하는가?
  - 유권자를 예로 들면, 전체 유권자 집단을 몇 개의 그룹으로 나눠 각 집단에 특성화된 선거전략을 세울 수 있다.
  - Labeling은 너무 비싼 작업이다.

## What is Clustering?
- 지도학습(Supervised Learning)
  - 학습 데이터에 예측 목표가 되는 속성이 포함 된 경우 (Label이 주어진 경우)
  - ex) 분류, 회귀분석
- 비지도학습(Unsupervised Learning)
  - 학습 데이터에 예측 목표값이 주어지지 않는다. (Label이 주어지지 않는다.)
  - 데이터에 바로 드러나지 않는 숨은 구조를 찾는 기술이다. (ex) 군집화, 차원축소)
    - 그 중 군집화는 데이터 항목의 유사도를 기반으로 비슷한 항목들을 묶어 그룹을 찾는 기법이다.
- 군집 분석(Cluster Analysis)
  - Data를 Subsets로 분리하는 과정이다.
  - 각 Subset을 Cluster이라고 하며, Cluster간에는 유사성이 없어야 한다.
  - Clustering Algorithm에 의해서 구현된다.

### Clustering Example
- Image Segmentation
  - Image를 유사한 부분끼리 군집화한다.
- Gene Expression Data

## Clustering
- Basic Idea : 유사한 객체들끼리 그룹화 시킬 것
- [예시 사진 삽입]

### Similarity Measure
- Distance Based : Data들이 떨어진 거리에 따라 묶는다.
  - Spherical Cluster을 만드는 경향이 있다.
  - ex) Euclidean, Road Network, Vector
  - Euclidean Distance : 두 점 사이를 잇는 직선 거리를 말한다.
  - Manhattan Distance : 두 점 사이의 x축거리 + y축거리가 된다.
  - Distance Measure은 아래와 같은 특성을 가져야 한다.
    - Symmetric : D(A, B) = D(B, A)
    - Positivity, Self-similarity : D(A, A) >= 0
    - Triangular Inequality : D(A, B) + D(B, C) >= D(A, C)
- Connectivity Based : Density를 고려해서 Clustering한다.
  - 임의의 모양을 갖는 Cluster을 형성한다.
  - ex) Density, Contiguity

### K-Means Clustering  
> 임의의 평균을 K개 정해 반복 연산을 통해 오차를 수정해가며 Clustering을 하는 작업이다.  
> 아래와 같은 과정으로 도식화 할 수 있다.  
![image](https://user-images.githubusercontent.com/71700079/162912077-e2b14dfe-73a8-4442-b58a-e340f4e06aa5.png)  

- Clustering 기법 중 Partitioning Method(분할법)에 속한다.
  - Partitioning Method : N개의 Data를 받았을 때 N보다 작거나 같은 K개의 Group으로 분할해서 Cluster을 형성하는 방법.
- K개의 평균을 쓰는데, K의 값은 사용자가 직접 부여해야한다.
- 아래와 같은 간단한 Iterative Algorithm에 따른다.  
![image](https://user-images.githubusercontent.com/71700079/162913329-881183a6-1b61-4075-8e48-be99c915b112.png)  

- 초기 Centroids는 보통 랜덤하게 정해지며, 만들어진 군집들의 평균으로 다시 설정된다.
- K-means는 적절한 Centroid가 찾아지면, 종료된다.
  - 웬만하면 수 회 안에 종료된다.
  - 초기에 Centroid를 잘못 설정해버리면, Clustering에 실패할 수도 있다.

### K-means Objective Function
> Centroid의 적합성을 판단하기 위해서 사용하는 수식인 SSE가 있다.  
- SSE(Sum of Squared Error) : Euclidean Distance를 이용해서 Centroid의 적합성을 판단한다.  
![image](https://user-images.githubusercontent.com/71700079/162921667-060c2b9b-f952-49e2-9ecc-567c7b9f526c.png)  
  - 이 SSE의 값이 작으면 작아질 수록 적합한 Centroid가 되는 것이다. (Centroid에 대해 많이 모여 있음을 의미)
 
### Limitation of K-means
- 초기 군집의 크기가 너무 다른 경우
- 초기 군집의 밀집도가 너무 다른경우
- 구형(Globular, Spherical)의 군집이 아닌 경우
  - 위의 모든 경우에는 K-means의 K를 늘려서 아래와 같이 해결할 수 있다.  
  ![image](https://user-images.githubusercontent.com/71700079/162922567-b8497eef-7c9f-4c99-9327-5d038bde503f.png)  
  ![image](https://user-images.githubusercontent.com/71700079/162922584-4c485493-5ed0-4be4-a026-8a9f2765ddbb.png)  

### Issues with K-menas
- K-means는 종종 Local optimal에서 종료된다.
- 적합한 K를 찾기 위해서 우리는 여러번 돌려 보아야 한다.
  - SSE(Sum of Squared Error)을 통해서 가장 적합한 K를 찾을 수 있다.
  - SSE 수치가 작을 수록 적합한 것!
- Nosiy Data나 Outlier에 매우 민감하다. (평균을 사용하기 때문)
- K-menas는 연속된 n차원 공간에서만 사용이 가능하다.

## Evaluation : Clustring
> 이전 시간에 했던 Classifier의 평가 지표에서 이어진다.  
- Supervised(External)
  - Jaccard Index = TP/(TP + FN + FP)
  - Rand index = (TP + TN) / N
- Unsupervised(Internal)
  - Cluster Cohesion : Cluster 내에서 얼마나 Centroid에 집중되어 있는가? SSE로 평가할 수 있다.
  - Cluster Separation : Cluster끼리 얼마나 잘 구별되어 있는가? SSB로 평가할 수 있다.   
  ![image](https://user-images.githubusercontent.com/71700079/162931459-b45e47ea-d1bf-4a6b-9ff3-30b7c280f65d.png)  
    - 위의 식에서, ```|Ci|```는 Cluster의 Size를 의미한다.
    - ```m - mi```는 전체 Cluster의 Centroid - Cluster의 Centroid를 의미한다.
  - Cluster Cohesion + Cluster Separation = K가 늘어나도 항상 같은 값으로 유지된다.
  - Silhouette coefficient
