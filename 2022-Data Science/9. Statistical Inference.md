# Statistical Inference
---
> 먼저 Data Science의 Process에는 어떤 것이 있었나?  

1. Ask a Question
2. Form a Hypothesis
3. Experiment, Observation, Analysis
4. Deploy and Evaluate (Loop Branch)
5. Make Decision / Scoring, Monitoring

- 위의 과정이 계속 반복되는 것이 Data Science이다.
- 1번과 2번 과정이 제대로 이루어지지 않으면, 뒤의 과정이 모두 꼬이게 된다.
  - 따라서 양질의 질문을 하고 양질의 가설을 세우는 것은 매우 중요한 일이다!

## Data and Sampling Distribution
> 통계적 추론이라 함은, 채취된 표본을 통해 거꾸로 모집단의 분포를 예측하는 것이다.  

- 추정 : 표본의 통계 측정치를 통해 모집단의 분포를 추정하는 것.
- 가설 검정 : 모수적 방법, 비모수적 방법이 존재

### Random Sampling
> 모집단 내의 선택 가능한 원소들을 무작위로 추출하는 과정.  
> 그 결과로 얻은 표본을 __단순 랜덤 표본__ 이라고 한다.  

- 표본추출
  - 비복원 추출 : 추출했던 원소를 다시 포함시키지 않는다. (표본의 크기가 작을 때 사용하면 위험)
  - 복원 추출 : 추출했던 원소를 다시 모집단에 포함시켜 재추출을 허용하는 추출.
- 표본의 편향(Bias) 정도에 따라 데이터의 품질이 달라지기 때문에, __대표성(Representiveness)__ 이라는 개념을 추가한다.
  - ex) 1,000만명을 상대로 조사를 했을 때 LANDON이 승리하리라 생각했지만, Roosvelt가 승리했다.
  - 이 대표성을 확보하기 위해, Random Sampling을 하는 것!

### Size vs Quality
> 언제 작은 데이터가 더 유리할까?  

- 아무리 Big Data의 시대라고 해도, 적은 데이터의 수가 더 유리한 경우가 있다.
  - Random Sampling에 시간과 노력을 기울일 수록 Bias가 줄어들고 Data의 품질이 매우 상승한다.
  - 몇 백만개의 Data 중에서 이를 하는 것은 매우 어려운 일이지만 수 천개의 Sample에서는 충분히 가능한 일이다.
- Data가 커서 유리한 경우는 무엇이 있을까?
  - Data가 크고 동시에 희박할 때이다.
  - Example : Google이 입력 받은 검색 쿼리를 처리하는 상황.
    - 발생 빈도가 매우 낮은 Query라도 사용자가 요청하면 응답을 해야만 한다.
    - 따라서 대부분이 0으로 가득 채워져있는 거대한 행렬을 Big Data로 갖고 있어야 제대로 응답이 가능하다.

## Big Data?
> Big Data의 __"Big"__ 은 절대값을 정의하는 것이 무의미한 상대적인 개념이다.  

- __"Big"__ 은 하나의 기계로 처리할 수 없을 때 적용한다.
  - Data가 너무 방대해서 하나의 컴퓨터로는 처리할 수 없을 때, Big Data라고 간주한다.
  - 시대에 따라 Big의 기준이 바뀌게 될 것이다.
