# Simple Linear Regression(단순 선형 회귀)
---
## Correlation
> 두 변수 사이의 연관성을 이해해보자.  

- Explanatory Variable(설명 변수) ==> 독립 변수
  - 아버지 키와 아들의 키가 궁금하다면? 아버지의 키는 설명 변수.
- Response Variable(반응 변수) ==> 종속 변수
  - 아들의 키는 반응 변수.
- 두 변수 간에는 어떤 관계가 있는가?
  - Scatter Plot(산점도)
  - Correlation Coefficient(상관 계수)
  - Linear Regression(선형 회귀)

### Drawing Scatter Plot
- X축 : Explanatory Variable
- Y축 : Response Variable
- In Python
  - Matplotlib : ```plt.scatter()```
  - Seaborn : ```sns.scatterplot()```, ```sns.regplot()```

### Describing Scatterplots
- Form : 관계가 선형인가 비선형인가?
- Direction : Positive한가 Negative 한가?(X가 증가할 때 Y의 양상)
- Strength : Strong? Moderate? Weak? Data가 모양을 그리는 정도
- Outliers : 일반적인 Pattern에서 크게 벗어나있는 Data

## Correlation Coefficient
- Pearson Correlation Coefficient
  - 두 개의 Data set에 대해서 Linear한 Correlation(Direction, Strength)을 측정한 값이다.
  - 음수 양수로서 Direction을 표현하고, 절대값으로 Strength를 표현한다.
  - Pearson's r이라고도 하며, Bivariate correlation이라고도 한다.
- Properties
  - 항상 -1 ~ 1 사이의 값을 갖는다.
  - Strong Positive Linear => 1에 가깝다.
  - Strong Negative Linear => -1에 가깝다.
  - Weaker Relationship을 갖는 경우 0에 가깝다.
  - 전혀 관계가 없다면 r = 0이 된다.

### 다양한 Coefficient
- Pearson Correlation Coefficient는, 평균 값을 식에 사용하기 때문에 Outlier에 영항을 많이 받는다.
  - 따라서 이상치에 Robust한 계수들이 개발됐다.
- Kendall's Tau
- Spearman's Rank Correlation(rho)
  - Pandas의 ```Corr()``` 함수에 Option으로 ```pearson, kendall, spearman```을 넣을 수 있다.

## Simple Linear Regression - Least Squares Regression Equations
- y = b0 + b1x : Simple Linear Regression Model
  - b0 : Intercept
  - b1 : Slope
- y ~ b0 + b1x + ei
  - ei(Epsilon i) : Error term
  - b1 : Rxy * (Sx / Sy)
  - b0 = y - b1x

### Least(Residual Sum of) Square Methods
- Residual(잔차)
  - Linear Graph와 Data의 y값 차이를 의미한다.
- 최소 제곱법(LSM)
  - 잔차의 제곱의 합을 최소로 만드는 방법을 말한다.
  - 최소의 Error를 만든다는 말이다.
