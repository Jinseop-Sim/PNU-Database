# Simple Linear Regression(단순 선형 회귀)
---
## Correlation
> 두 변수 사이의 연관성을 이해해보자.  

- Explanatory Variable(설명 변수) ==> 독립 변수
  - 아버지 키와 아들의 키가 궁금하다면? 아버지의 키는 설명 변수.
- Response Variable(반응 변수) ==> 종속 변수
  - 아들의 키는 반응 변수.
- 두 변수 간에는 어떤 관계가 있는가?
  - Scatter Plot(산점도)
  - Correlation Coefficient(상관 계수)
  - Linear Regression(선형 회귀)

### Drawing Scatter Plot
- X축 : Explanatory Variable
- Y축 : Response Variable
- In Python
  - Matplotlib : ```plt.scatter()```
  - Seaborn : ```sns.scatterplot()```, ```sns.regplot()```

### Describing Scatterplots
- Form : 관계가 선형인가 비선형인가?
- Direction : Positive한가 Negative 한가?(X가 증가할 때 Y의 양상)
- Strength : Strong? Moderate? Weak? Data가 모양을 그리는 정도
- Outliers : 일반적인 Pattern에서 크게 벗어나있는 Data

## Correlation Coefficient
- Pearson Correlation Coefficient
  - 두 개의 Data set에 대해서 Linear한 Correlation(Direction, Strength)을 측정한 값이다.
  - 음수 양수로서 Direction을 표현하고, 절대값으로 Strength를 표현한다.
  - Pearson's r이라고도 하며, Bivariate correlation이라고도 한다.
- Properties
  - 항상 -1 ~ 1 사이의 값을 갖는다.
  - Strong Positive Linear => 1에 가깝다.
  - Strong Negative Linear => -1에 가깝다.
  - Weaker Relationship을 갖는 경우 0에 가깝다.
  - 전혀 관계가 없다면 r = 0이 된다.

### 다양한 Coefficient
- Pearson Correlation Coefficient는, 평균 값을 식에 사용하기 때문에 Outlier에 영항을 많이 받는다.
  - 따라서 이상치에 Robust한 계수들이 개발됐다.
- Kendall's Tau
- Spearman's Rank Correlation(rho)
  - Pandas의 ```Corr()``` 함수에 Option으로 ```pearson, kendall, spearman```을 넣을 수 있다.

## Simple Linear Regression - Least Squares Regression Equations
- y = b0 + b1x : Simple Linear Regression Model
  - b0 : Intercept
  - b1 : Slope
- y ~ b0 + b1x + ei
  - ei(Epsilon i) : Error term
  - b1 : Rxy * (Sx / Sy)
  - b0 = y - b1x

### Least(Residual Sum of) Square Methods
- Residual(잔차)
  - Linear Graph와 Data의 y값 차이를 의미한다.
- 최소 제곱법(LSM)
  - 잔차의 제곱의 합을 최소로 만드는 방법을 말한다.
  - 최소의 Error를 만든다는 말이다.

### Coefficient of Determination : r^2
> 어떤 Data들이, 이 Model의 적합도를 잘 나타내는지 말해준다.  

- r^2 : 결정 계수
  - 모형 적합도를 표현하는 값이다.
  - Prediction Error가 얼마나 제거되었는 지를 표현한다. (최소제곱회귀 내에서)
  - SST : Total sum of squares ((Data의 y값 - y값 평균)의 제곱)
  - SSR : Regression sum of squares ((y Predicted value - y값 평균)의 제곱)
  - SSE : Error sum of Squares ((Data의 y값 - y Predicted value)의 제곱)
  - 이 때 r^2 = SSR/SST이고, SST = SSR + SSE
    - 따라서 r^2 = (SST-SSE)/SST = 1 - SSE/SST
- Intuition(직관) of r^2
  - Regression을 사용하지 않을 때에는, 단순히 Average를 통해서 SST값을 내어 Error 정도를 예측할 수 있다.
  - Regression을 사용할 때에는?
    - SSE를 계산해서 Error의 정도를 예측할 수 있다.
    - 훨씬 Error가 줄어든다!
    - 이 때, SST - SSE / SST ==> 즉, r^2은 회귀를 했을 때와 안했을 때 얼마나 Error 값이 줄어드는 지를 의미한다!
