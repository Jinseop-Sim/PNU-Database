# k-Nearest Neighbor
---
> 이전에 학습한 Clustering은 비지도학습(Unsupervisored)의 일종이었다.  
> 이번에 배울 k-Nearest Neighbor은 Training Data에 Lable이 존재하는 지도 학습의 일종이다.  

## Instance-Based Classifier  

- Lazy Learning : Model을 따로 만들어 학습하지 않고, 그냥 Unseen Case를 받아서 Class를 예측한다.
- Eager Learning : Training Data를 통해 Classification Model을 만들어서 학습을 한 뒤 Unseen Case의 Class를 예측한다.

## Nearest Neighbors Classifiers
> 근접 이웃 분류기.  

- Basic Idea 
  - 어떤 새로운 동물이 나타났을 때, 오리처럼 걷고 오리처럼 꽥꽥 소리를 낸다. ==> 오리일 가능성이 높다!
  - 각각의 Instance들과 Distance를 계산해서, K개의 Nearest Record를 뽑아낸다.
- Requirements
  - Set of Stored Records (with Label)
  - Distance Metric(Euclidean, Manhattan..)
  - Value of K(Number of Nearest Neighbors)
- Tie Break의 경우에는, Random으로 뽑거나, 거리에 따른 Weight를 주어서 더 높은 Class의 쪽으로!

### 1-Nearest Neighbor : Voronoi Diagram  
![image](https://user-images.githubusercontent.com/71700079/163662149-6b4d60ba-8118-451e-81f8-5cd6454033fe.png)  

## Issues
- Scaling Issue
  - 종종 Preprocessing이 필요한 경우가 있다.
  - ex) Euclidean Distance를 사용하는 경우, Height, Weight, Income ==> 범위가 모두 제각각이다.
  - sol) Mahalanobis Distance를 이용한다.
- Choosing the Value of the K
  - K가 너무 작으면, Noise points(e.g. Outlier)에 영향을 많이 받아버린다.
  - K가 너무 커버리면, Neighborhood가  
- Curse of Dimensionality
  - Low-Dimensional : 근접 이웃이 평균 거리에 비해 충분히 가깝다.
  - High-Dimensional : 차원의 높아질 수록, Data들 간의 평균 거리가 멀어진다.
    - 즉, 근접한 이웃을 뽑았다고 하더라도 그렇게 가까운 이웃이 아니라는 것.
